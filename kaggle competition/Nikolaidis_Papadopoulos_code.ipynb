{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d95bba3",
      "metadata": {
        "id": "9d95bba3"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import os\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "# deep learning libraries\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import class_weight\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras import applications\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D, Activation, Flatten, Dense, Dropout, BatchNormalization\n",
        "from keras.preprocessing import image\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "from tensorflow.keras.applications import VGG19\n",
        "from tensorflow.keras.models import Model\n",
        "from keras.optimizers import SGD\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.models import load_model\n",
        "from PIL import Image\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K9I_sgjYkauY",
      "metadata": {
        "id": "K9I_sgjYkauY"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bX-IfylS-jIP",
      "metadata": {
        "id": "bX-IfylS-jIP"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['KAGGLE_USERNAME']=\"symeonp\"\n",
        "os.environ['KAGGLE_KEY']=\"c72551e3c216704f2f2e648d152e71d2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KC5r0flq-jKv",
      "metadata": {
        "id": "KC5r0flq-jKv"
      },
      "outputs": [],
      "source": [
        "!kaggle competitions download -c detect-pneumonia-spring-2023"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4x7U2oj1-jMv",
      "metadata": {
        "id": "4x7U2oj1-jMv"
      },
      "outputs": [],
      "source": [
        "!unzip detect-pneumonia-spring-2023.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "trgTtbsW_LCt",
      "metadata": {
        "id": "trgTtbsW_LCt"
      },
      "outputs": [],
      "source": [
        "# Path to train images\n",
        "train_dir = \"/content/train_images/train_images\"\n",
        "\n",
        "# Path to test images\n",
        "test_dir = \"/content/test_images/test_images\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "844EQNjO_iJ_",
      "metadata": {
        "id": "844EQNjO_iJ_"
      },
      "outputs": [],
      "source": [
        "# Transform the csv file into a Pandas DataFrame\n",
        "labels = pd.read_csv('labels_train.csv')\n",
        "\n",
        "# Print the first few rows to check\n",
        "print(labels.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6559ef90",
      "metadata": {
        "id": "6559ef90"
      },
      "outputs": [],
      "source": [
        "#Function for loading images\n",
        "\n",
        "def load_image(file_name, image_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Load an image from the train_images directory, given its file name.\n",
        "    Resize the image to the given size and normalize pixel values.\n",
        "    \"\"\"\n",
        "    # Full path to the train_images directory\n",
        "    train_images_path = \"/content/train_images/train_images\"\n",
        "\n",
        "    # Build the path to the image file\n",
        "    image_path = os.path.join(train_images_path, file_name)\n",
        "\n",
        "    # Print the image path to check\n",
        "    print(f\"Loading image from {image_path}\")\n",
        "\n",
        "    # Load the image using OpenCV\n",
        "    image = cv2.imread(image_path,cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "    # Convert grayscale image to RGB\n",
        "    image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "    # Check if the image was loaded properly\n",
        "    if image is None:\n",
        "        print(f\"Failed to load image from {image_path}\")\n",
        "        return None\n",
        "\n",
        "    # Resize the image\n",
        "    image = cv2.resize(image, image_size)\n",
        "\n",
        "    # Normalize pixel values to [0, 1]\n",
        "    image = image / 255.0\n",
        "\n",
        "    return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cee43d27",
      "metadata": {
        "id": "cee43d27"
      },
      "outputs": [],
      "source": [
        "#Function for creating the dataset\n",
        "\n",
        "def create_dataset(file_names, labels, image_size=(224, 224)):\n",
        "    \"\"\"\n",
        "    Create a dataset of images and labels.\n",
        "    \"\"\"\n",
        "    # Initialize lists to hold images and labels\n",
        "    images = []\n",
        "    label_values = []\n",
        "\n",
        "    # Iterate through the image file names and load each image\n",
        "    for file_name in file_names:\n",
        "        image = load_image(file_name, image_size)\n",
        "\n",
        "        # Skip to the next image if current image failed to load\n",
        "        if image is None:\n",
        "            continue\n",
        "\n",
        "        label = labels.loc[labels['file_name'] == file_name, 'class_id'].values[0]\n",
        "\n",
        "        images.append(image)\n",
        "        label_values.append(label)\n",
        "\n",
        "    # Convert images and labels to numpy arrays\n",
        "    images = np.array(images)\n",
        "    label_values = np.array(label_values)\n",
        "\n",
        "    return images, label_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4231d04",
      "metadata": {
        "id": "b4231d04"
      },
      "outputs": [],
      "source": [
        "# Create the dataset\n",
        "images, labels = create_dataset(labels['file_name'], labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d75cf58b",
      "metadata": {
        "id": "d75cf58b"
      },
      "outputs": [],
      "source": [
        "# Plotting sample images from the train dataset\n",
        "fig, axes = plt.subplots(2, 5, figsize=(10, 5))\n",
        "\n",
        "for i, ax in enumerate(axes.flat):\n",
        "    ax.imshow(images[i])\n",
        "    ax.set_title(f'Label: {labels[i]}')\n",
        "    ax.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55cf04b4",
      "metadata": {
        "id": "55cf04b4"
      },
      "outputs": [],
      "source": [
        "# Split data into train and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "# Estimate sample weights by class for unbalanced datasets.\n",
        "class_weights = class_weight.compute_sample_weight(class_weight='balanced', y=y_train)\n",
        "class_weights_dict = dict(enumerate(class_weights))\n",
        "\n",
        "# Dimensions of our images.\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# Define a callback to save the model weights only when validation accuracy is maximized\n",
        "# and another callback for earlystopping\n",
        "checkpoint = ModelCheckpoint('best_model_test.h5', monitor='val_accuracy', mode='max', save_best_only=True, verbose=1)\n",
        "callbacks = EarlyStopping(monitor='val_loss', patience=40, verbose=1, mode='auto')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64ff61f1",
      "metadata": {
        "id": "64ff61f1"
      },
      "outputs": [],
      "source": [
        "# This is the augmentation configuration we will use for training\n",
        "\n",
        "# Function for adjusting contrast\n",
        "def custom_augmentation(np_tensor):\n",
        "\n",
        "  def random_contrast(np_tensor):\n",
        "    return tf.image.random_contrast(np_tensor, 0.5, 2)\n",
        "\n",
        "  augmnted_tensor = random_contrast(np_tensor)\n",
        "  return np.array(augmnted_tensor)\n",
        "\n",
        "\n",
        "# Use ImageDataGenerator to generate batches of tensor image data with real-time data augmentation.\n",
        "train_datagen = ImageDataGenerator(\n",
        "    shear_range=0.2,            # randomly apply shearing transformations\n",
        "    zoom_range=0.2,             # randomly zoom inside pictures\n",
        "    horizontal_flip=False,       # randomly flip images horizontally\n",
        "    width_shift_range=0.1,      # randomly shift images horizontally (fraction of total width)\n",
        "    height_shift_range=0.1     # randomly shift images vertically (fraction of total height)\n",
        ")\n",
        "\n",
        "# Prepare generators for training and validation sets\n",
        "batch_size = 32\n",
        "train_generator = train_datagen.flow(X_train, y_train, batch_size=batch_size)\n",
        "val_generator = ImageDataGenerator().flow(X_val, y_val, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8cd084d",
      "metadata": {
        "id": "c8cd084d"
      },
      "outputs": [],
      "source": [
        "# Plot sample images from train dataset after image augmentation\n",
        "image_batch, label_batch = next(iter(train_generator))\n",
        "\n",
        "def show_batch(image_batch, label_batch):\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    for n in range(15):\n",
        "        ax = plt.subplot(5, 5, n + 1)\n",
        "        plt.imshow(image_batch[n])\n",
        "        if label_batch[n] == 1:\n",
        "            plt.title(\"BACT PNEUMONIA\")\n",
        "        elif label_batch[n] == 2:\n",
        "            plt.title(\"VIR PNEUMONIA\")\n",
        "        else:\n",
        "            plt.title(\"NORMAL\")\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "show_batch(image_batch, label_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06627df2",
      "metadata": {
        "id": "06627df2"
      },
      "outputs": [],
      "source": [
        "# Load the VGG19 pre-training on ImageNet with weights pre-trained  on ImageNet,\n",
        "# without the top layers and with the desired input shape\n",
        "base_model = VGG19(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Flatten the output layer to 1 dimension\n",
        "x = Flatten()(base_model.output)\n",
        "\n",
        "# Add two fully connected layers with 256 hidden units and ReLU activation\n",
        "# followed by a BatchNormalization and a Dropout layer\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x= BatchNormalization()(x)\n",
        "x=Dropout(0.7)(x)\n",
        "x= Dense(256,activation='relu')(x)\n",
        "x= BatchNormalization()(x)\n",
        "x=Dropout(0.7)(x)\n",
        "\n",
        "# Add a final softmax layer for classification\n",
        "output = Dense(3, activation='softmax')(x)\n",
        "\n",
        "# Define the model object that will be used for training\n",
        "model = Model(base_model.input, output)\n",
        "\n",
        "# Freeze the layers of the base model.\n",
        "#This is important so that the weights of the pre-trained model remain unchanged during the initial training.\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Compile the model using Adam optimizer, sparse_categorical_crossentropy as the loss function\n",
        "# and accuracy as the evaluation metric\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Fit the model for 50 epochs\n",
        "history = model.fit(train_generator,\n",
        "                        epochs=50,\n",
        "                        validation_steps=len(X_val) // batch_size,\n",
        "                        validation_data=val_generator,\n",
        "                        steps_per_epoch=len(X_train) // batch_size,\n",
        "                        callbacks = [callbacks,checkpoint]\n",
        "                        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dd33d26",
      "metadata": {
        "id": "5dd33d26"
      },
      "outputs": [],
      "source": [
        "# Adjustments, recompile and refit of the model\n",
        "\n",
        "# Unfreeze the layers\n",
        "for layer in base_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Use SGD with momentum as the optimizer, and add a learning rate scheduler\n",
        "optimizer = SGD(lr=0.0001, momentum=0.9)\n",
        "lr_scheduler = ReduceLROnPlateau(factor=0.1, patience=10)\n",
        "\n",
        "# Recompile the model with the new optimizer and learning rate scheduler\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Add Batch Normalization after each Conv layer in the base model\n",
        "for i, layer in enumerate(base_model.layers):\n",
        "    if isinstance(layer, Conv2D):\n",
        "        base_model.layers.insert(i+1, BatchNormalization())\n",
        "\n",
        "# Adjust data augmentation settings\n",
        "train_datagen = ImageDataGenerator(\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=False,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1\n",
        ")\n",
        "\n",
        "# Fit again the model for another 50 epochs with same batch size and number of steps adding a learning rate scheduler\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=50,\n",
        "                    validation_steps=len(X_val) // batch_size,\n",
        "                    validation_data=val_generator,\n",
        "                    steps_per_epoch=len(X_train) // batch_size,\n",
        "                    callbacks = [callbacks, checkpoint, lr_scheduler]  # Add the learning rate scheduler to the callbacks\n",
        "                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MNaRZ99_froN",
      "metadata": {
        "id": "MNaRZ99_froN"
      },
      "outputs": [],
      "source": [
        "# Loading the best weights, making predictions on the test images and generating the final .csv file\n",
        "\n",
        "# Load the best model\n",
        "model = load_model('best_model_test.h5')  # replace with the path to your saved model\n",
        "\n",
        "# Define the image dimensions (must be the same as what the model expects)\n",
        "img_width, img_height = 224, 224\n",
        "\n",
        "# Directory containing test images\n",
        "test_dir = \"/content/test_images/test_images\"\n",
        "\n",
        "# DataFrame to store results\n",
        "results = []\n",
        "\n",
        "# Loop over each file in the test directory\n",
        "for file in os.listdir(test_dir):\n",
        "    # Load the image file\n",
        "    img_path = os.path.join(test_dir, file)\n",
        "    img = Image.open(img_path).convert('RGB')  # Convert to RGB\n",
        "    img = img.resize((img_width, img_height), Image.NEAREST)  # Resize the image\n",
        "\n",
        "    # Convert the image to a numpy array and reshape it\n",
        "    x = image.img_to_array(img)\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "\n",
        "    # Normalize the image\n",
        "    x /= 255.0\n",
        "\n",
        "    # Use the model to make a prediction\n",
        "    prediction = model.predict(x)\n",
        "    predicted_class = np.argmax(prediction)\n",
        "\n",
        "    # Add the filename and prediction to the results\n",
        "    results.append({'file_name': file, 'class_id': predicted_class})\n",
        "\n",
        "# Convert results to a DataFrame and save as a CSV file\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('test_predictionsvgg19.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HZnhVUmoh7JX",
      "metadata": {
        "id": "HZnhVUmoh7JX"
      },
      "outputs": [],
      "source": [
        "# Plots\n",
        "\n",
        "# Plotting the loss of the model\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plotting the accuracy of the model\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}